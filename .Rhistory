library(igraph)
library(ggraph)
library(forcats)
source("ladowanie.R")
source("przygotowanie.R")
source("przygotowanie.R")
paths <- c("C:/Users/Joanna/Desktop/programowanie/r/papers/Faliszewski.pdf", "C:/Users/Joanna/Desktop/programowanie/r/papers/Garey.pdf", "C:/Users/Joanna/Desktop/programowanie/r/papers/Gerya.pdf")
pdf_files <- loading_pdf_files(paths = paths)
View(loading_pdf_files)
pdf_files <- loading_pdf_files(paths = paths)
source("ladowanie.R")
pdf_files <- loading_pdf_files(paths = paths)
pdf_files
pdf_files <- preprocessing_of_pdf_files(pdf_files = pdf_files)
pdf_files
dim(pdf_files)
pdf_files <- loading_pdf_files(paths = paths)
dim(pdf_files)
pdf_files
length(pdf_files)
pdf_files <- preprocessing_of_pdf_files(pdf_files = pdf_files)
length(pdf_files)
typeof(pdf_files)
source("przygotowanie.R")
pdf_files <- loading_pdf_files(paths = paths)
pdf_files <- preprocessing_of_pdf_files(pdf_files = pdf_files)
typeof(pdf_files)
length(pdf_files)
source("przygotowanie.R")
pdf_files <- loading_pdf_files(paths = paths)
pdf_files <- preprocessing_of_pdf_files(pdf_files = pdf_files)
length(pdf_files)
pdf_files
source("przygotowanie.R")
pdf_files <- loading_pdf_files(paths = paths)
pdf_files <- preprocessing_of_pdf_files(pdf_files = pdf_files)
source("przygotowanie.R")
pdf_files <- loading_pdf_files(paths = paths)
pdf_files <- preprocessing_of_pdf_files(pdf_files = pdf_files)
length(pdf_files)
pdf_files
source("przygotowanie.R")
pdf_files <- loading_pdf_files(paths = paths)
pdf_files <- preprocessing_of_pdf_files(pdf_files = pdf_files)
length(pdf_files)
pdf_files
pdf_files <- loading_pdf_files(paths = paths)
preprocessed_pdf_files <- preprocessing_of_pdf_files(pdf_files = pdf_files)
word_counts <- character()
for (pdf_file in preprocessed_pdf_files){
x <- pdf_file %>% count(word, sort = TRUE)
word_counts <- append(word_counts, x)
}
View(preprocessed_pdf_files)
for (pdf_file in preprocessed_pdf_files){
x <- pdf_file %>% count(word, sort = TRUE)
#word_counts <- append(word_counts, x)
print(x)
print(typeof(x))
}
pdf_file_unlist <- unlist(strsplit(pdf_file, " "))
pdf_file_lemmatized <- lemmatize_words(pdf_file_unlist)
pdf_file_tokenized <- tibble(text = pdf_file_lemmatized) %>% unnest_tokens(word, text)
stop_words <- stopwords("en") %>% tibble(word = .)
pdf_file_tokenized <- anti_join(pdf_file_tokenized, stop_words, by = "word")
typeof(pdf_file_tokenized)
typeof(pdf_files[1])
source("przygotowanie.R")
pdf_files <- loading_pdf_files(paths = paths)
preprocessed_pdf_files <- preprocessing_of_pdf_files(pdf_files = pdf_files)
word_counts <- pdf_file_tokenized %>% count(word, sort = TRUE)
typeof(word_counts)
word_counts <- list()
for (pdf_file in preprocessed_pdf_files){
x <- pdf_file %>% count(word, sort = TRUE)
word_counts <- append(word_counts, x)
print(typeof(x))
}
for (pdf_file in preprocessed_pdf_files){
x <- pdf_file %>% count(word, sort = TRUE)
#word_counts <- append(word_counts, x)
print(typeof(x))
}
for (pf in preprocessed_pdf_files){
x <- pf %>% count(word, sort = TRUE)
word_counts <- append(word_counts, x)
print(typeof(x))
}
pf
View(stop_words)
View(word_counts)
View(word_counts)
pdf_file <- pdf_text("C:/Users/Joanna/Desktop/programowanie/r/papers/Faliszewski.pdf")
pdf_file <- tolower(gsub("[\r\n]", " ", paste(pdf_file, collapse=" ")))
pdf_file <- removePunctuation(pdf_file)
pdf_file <- removeNumbers(pdf_file)
pdf_file_unlist <- unlist(strsplit(pdf_file, " "))
pdf_file_lemmatized <- lemmatize_words(pdf_file_unlist)
pdf_file_tokenized <- tibble(text = pdf_file_lemmatized) %>% unnest_tokens(word, text)
stop_words <- stopwords("en") %>% tibble(word = .)
pdf_file_tokenized <- anti_join(pdf_file_tokenized, stop_words, by = "word")
word_counts <- pdf_file_tokenized %>% count(word, sort = TRUE)
typeof(pdf_file_tokenized)
typeof(word_counts)
typeof(word_counts[1])
typeof(preprocessed_pdf_files[1])
preprocessed_pdf_files[1]
word_counts <- list()
for (i in 1:length(preprocessed_pdf_files)){
x <- preprocessed_pdf_files[i] %>% count(word, sort = TRUE)
word_counts <- append(word_counts, x)
print(typeof(x))
}
for (i in 1:length(preprocessed_pdf_files)){
#x <- preprocessed_pdf_files[i] %>% count(word, sort = TRUE)
#word_counts <- append(word_counts, x)
print(preprocessed_pdf_files[i])
print(typeof(preprocessed_pdf_files[i]))
}
for (i in 1:length(preprocessed_pdf_files)){
x <- preprocessed_pdf_files[i] %>% count(word, sort = TRUE)
word_counts <- append(word_counts, x)
}
class(preprocessed_pdf_files[1])
class(pdf_file_tokenized)
source("przygotowanie.R")
pdf_files <- loading_pdf_files(paths = paths)
preprocessed_pdf_files <- preprocessing_of_pdf_files(pdf_files = pdf_files)
word_counts <- list()
for (i in 1:length(preprocessed_pdf_files)){
x <- preprocessed_pdf_files[i] %>% count(word, sort = TRUE)
word_counts <- append(word_counts, x)
}
word_counts <- tibble()
for (i in 1:length(preprocessed_pdf_files)){
x <- preprocessed_pdf_files[i] %>% count(word, sort = TRUE)
word_counts <- append(word_counts, x)
}
class(preprocessed_pdf_files[1])
for (i in 1:length(preprocessed_pdf_files)){
x <- as_tibble(preprocessed_pdf_files[i]) %>% count(word, sort = TRUE)
word_counts <- append(word_counts, x)
}
word_counts
View(word_counts)
word_counts[1]
as_tibble(word_counts[1])
for (i in 1:length(preprocessed_pdf_files)){
x <- as_tibble(preprocessed_pdf_files[i]) %>% count(word, sort = TRUE)
print(x)
word_counts <- append(word_counts, x)
}
as_tibble(word_counts)
length(word_counts)
View(word_counts)
word_counts <- tibble()
for (i in 1:length(preprocessed_pdf_files)){
x <- as_tibble(preprocessed_pdf_files[i]) %>% count(word, sort = TRUE)
print(x)
word_counts <- append(word_counts, x)
}
length(word_counts)
word_counts <- tibble()
for (i in 1:length(preprocessed_pdf_files)){
x <- as_tibble(preprocessed_pdf_files[i]) %>% count(word, sort = TRUE)
word_counts[[i]] <- x
}
library(tibble)
word_counts <- list()
for (i in 1:length(preprocessed_pdf_files)){
word_counts[[i]] <- tibble(id=i, word = paste0("word", i), count = count(word, sort = TRUE))
}
for (i in 1:length(preprocessed_pdf_files)){
word_counts[[i]] <- tibble(text = preprocessed_pdf_files[[i]]) %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE)
}
word_counts
source("przygotowanie.R")
pdf_files <- loading_pdf_files(paths = paths)
preprocessed_pdf_files <- preprocessing_of_pdf_files(pdf_files = pdf_files)
word_counts <- list()
for (i in 1:length(preprocessed_pdf_files)){
word_counts[[i]] <- tibble(text = preprocessed_pdf_files[[i]]) %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE)
}
library(pdftools)
library(tm)
library(tidytext)
library(ggplot2)
library(wordcloud)
library(wordcloud2)
library(RColorBrewer)
library(tokenizers)
library(hash)
library(stringr)
library(dplyr)
library(textstem)
library(tidyverse)
library(igraph)
library(ggraph)
library(forcats)
library(tibble)
source("ladowanie.R")
source("przygotowanie.R")
paths <- c("C:/Users/Joanna/Desktop/programowanie/r/papers/Faliszewski.pdf", "C:/Users/Joanna/Desktop/programowanie/r/papers/Garey.pdf", "C:/Users/Joanna/Desktop/programowanie/r/papers/Gerya.pdf")
pdf_files <- loading_pdf_files(paths = paths)
preprocessed_pdf_files <- preprocessing_of_pdf_files(pdf_files = pdf_files)
word_counts <- list()
for (i in 1:length(preprocessed_pdf_files)){
word_counts[[i]] <- tibble(text = preprocessed_pdf_files[[i]]) %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE)
}
library(topicmodels)
library(textdata)
dtm <- word_counts %>% cast_dtm(document = "word", term = "word", value = "n")
k <- 2
data("AssociatedPress")
lda_model <- LDA(dtm, k = k, control = list(seed = 1234))
dtm <- word_counts %>% cast_dtm(document = "word", term = "word", value = "n")
View(word_counts)
word_counts_df <- bind_rows(word_counts, .id = "document")
dtm <- word_counts_df %>% cast_dtm(document = "word", term = "word", value = "n")
lda_model <- LDA(dtm, k = k, control = list(seed = 1234))
terms(lda_model, 10)
topics <- tidy(lda_model, matrix = "beta")
topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
ggplot(aes(beta, fct_reorder(term, beta), fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
labs(x = "Beta", y = NULL)
View(word_counts_df)
k <- 5
lda_model <- LDA(dtm, k = k, control = list(seed = 1234))
terms(lda_model, 10)
topics <- tidy(lda_model, matrix = "beta")
topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
ggplot(aes(beta, fct_reorder(term, beta), fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
labs(x = "Beta", y = NULL)
source("LDA.R")
source("LDA.R")
library(pdftools)
library(tm)
library(tidytext)
library(tidyverse)
library(ggplot2)
library(wordcloud)
library(wordcloud2)
library(RColorBrewer)
library(tokenizers)
library(hash)
library(stringr)
library(dplyr)
library(textstem)
library(igraph)
library(ggraph)
library(tibble)
source("loading_files.R")
getwd()
setwd("C:/Users/Joanna/Desktop/programowanie/r/nlp-for-papers")
source("loading_files.R")
source("preprocessing.R")
paths <- c("C:/Users/Joanna/Desktop/programowanie/r/papers/Faliszewski.pdf", "C:/Users/Joanna/Desktop/programowanie/r/papers/Garey.pdf", "C:/Users/Joanna/Desktop/programowanie/r/papers/Gerya.pdf")
pdf_files <- loading_pdf_files(paths = paths)
pdf_files <- preprocessing_of_pdf_files(pdf_files = pdf_files)
word_counts <- list()
for (i in 1:length(preprocessed_pdf_files)){
word_counts[[i]] <- tibble(text = preprocessed_pdf_files[[i]]) %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE)
}
pdf_files <- loading_pdf_files(paths = paths)
preprocessed_pdf_files <- preprocessing_of_pdf_files(pdf_files = pdf_files)
word_counts <- list()
for (i in 1:length(preprocessed_pdf_files)){
word_counts[[i]] <- tibble(text = preprocessed_pdf_files[[i]]) %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE)
}
source("LDA.R")
LDA_modelling(word_counts)
word_counts_df <- bind_rows(word_counts, .id = "document")
dtm <- word_counts_df %>% cast_dtm(document = "word", term = "word", value = "n")
View(dtm)
View(dtm)
pdf_files <- loading_pdf_files(paths = paths)
preprocessed_pdf_files <- preprocessing_of_pdf_files(pdf_files = pdf_files, bigrams = FALSE)
source("przygotowanie.R")
setwd("C:/Users/Joanna/Desktop/programowanie/r")
source("przygotowanie.R")
pdf_files <- loading_pdf_files(paths = paths)
preprocessed_pdf_files <- preprocessing_of_pdf_files(pdf_files = pdf_files, bigrams = FALSE)
word_counts <- list()
for (i in 1:length(preprocessed_pdf_files)){
word_counts[[i]] <- tibble(text = preprocessed_pdf_files[[i]]) %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE)
}
word_counts_df <- bind_rows(word_counts, .id = "document")
dtm <- word_counts_df %>% cast_dtm(document = "word", term = "word", value = "n")
lda_model <- LDA(dtm, k = k, control = list(seed = 1234))
k <- 5
lda_model <- LDA(dtm, k = k, control = list(seed = 1234))
terms(lda_model, 10)
topics <- tidy(lda_model, matrix = "beta")
topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
ggplot(aes(beta, fct_reorder(term, beta), fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
labs(x = "Beta", y = NULL)
preprocessed_pdf_files_bigrams <- preprocessing_of_pdf_files(pdf_files = pdf_files, bigrams = TRUE)
source("przygotowanie.R")
pdf_files <- loading_pdf_files(paths = paths)
preprocessed_pdf_files <- preprocessing_of_pdf_files(pdf_files = pdf_files, bigrams = FALSE)
word_counts <- list()
for (i in 1:length(preprocessed_pdf_files)){
word_counts[[i]] <- tibble(text = preprocessed_pdf_files[[i]]) %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE)
}
word_counts_df <- bind_rows(word_counts, .id = "document")
dtm <- word_counts_df %>% cast_dtm(document = "word", term = "word", value = "n")
# LDA training
k <- 5
lda_model <- LDA(dtm, k = k, control = list(seed = 1234))
terms(lda_model, 10)
topics <- tidy(lda_model, matrix = "beta")
topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
ggplot(aes(beta, fct_reorder(term, beta), fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
labs(x = "Beta", y = NULL)
source("przygotowanie.R")
source("przygotowanie.R")
source("przygotowanie.R")
pdf_files <- loading_pdf_files(paths = paths)
preprocessed_pdf_files <- preprocessing_of_pdf_files(pdf_files = pdf_files, bigrams = FALSE)
preprocessed_pdf_files_bigrams <- preprocessing_of_pdf_files(pdf_files = pdf_files, bigrams = TRUE)
View(preprocessed_pdf_files_bigrams)
View(preprocessed_pdf_files)
View(preprocessed_pdf_files_bigrams)
word_counts <- list()
for (i in 1:length(preprocessed_pdf_files)){
word_counts[[i]] <- tibble(text = preprocessed_pdf_files[[i]]) %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE)
}
word_counts_df <- bind_rows(word_counts, .id = "document")
dtm <- word_counts_df %>% cast_dtm(document = "word", term = "word", value = "n")
k <- 5
lda_model <- LDA(dtm, k = k, control = list(seed = 1234))
terms(lda_model, 10)
topics <- tidy(lda_model, matrix = "beta")
topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
ggplot(aes(beta, fct_reorder(term, beta), fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
labs(x = "Beta", y = NULL)
setwd("C:/Users/Joanna/Desktop/programowanie/r/nlp-for-papers")
pdf_files <- loading_pdf_files(paths = paths)
preprocessed_pdf_files <- preprocessing_of_pdf_files(pdf_files = pdf_files, bigrams = FALSE)
source("preprocessing.R")
pdf_files <- loading_pdf_files(paths = paths)
preprocessed_pdf_files <- preprocessing_of_pdf_files(pdf_files = pdf_files, bigrams = FALSE)
word_counts <- list()
for (i in 1:length(preprocessed_pdf_files)){
word_counts[[i]] <- tibble(text = preprocessed_pdf_files[[i]]) %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE)
}
LDA_modelling(word_counts)
setwd("C:/Users/Joanna/Desktop/programowanie/r")
source("przygotowanie.R")
pdf_files <- loading_pdf_files(paths = paths)
preprocessed_pdf_files <- preprocessing_of_pdf_files(pdf_files = pdf_files, bigrams = FALSE)
preprocessed_pdf_files_bigrams <- preprocessing_of_pdf_files(pdf_files = pdf_files, bigrams = TRUE)
word_counts <- list()
for (i in 1:length(preprocessed_pdf_files)){
word_counts[[i]] <- tibble(text = preprocessed_pdf_files[[i]]) %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE)
}
bigram_counts <- list()
for (i in 1:length(preprocessed_pdf_files_bigrams)){
word_counts[[i]] <- tibble(text = preprocessed_pdf_files_bigrams[[i]]) %>%
unnest_tokens(bigram, text) %>%
count(bigram, sort = TRUE)
}
View(word_counts)
View(bigram_counts)
View(preprocessed_pdf_files_bigrams)
View(preprocessed_pdf_files)
View(preprocessed_pdf_files_bigrams)
for (i in 1:length(preprocessed_pdf_files_bigrams)){
word_counts[[i]] <- tibble(text = preprocessed_pdf_files_bigrams[[i]]) %>%
unnest_tokens(bigram, text) %>%
count(bigram, sort = TRUE)
}
View(bigram_counts)
word_counts <- list()
for (i in 1:length(preprocessed_pdf_files)){
word_counts[[i]] <- tibble(text = preprocessed_pdf_files[[i]]) %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE)
}
bigram_counts <- list()
for (i in 1:length(preprocessed_pdf_files_bigrams)){
bigram_counts[[i]] <- tibble(text = preprocessed_pdf_files_bigrams[[i]]) %>%
unnest_tokens(bigram, text) %>%
count(bigram, sort = TRUE)
}
View(bigram_counts)
bigram_counts_df <- bind_rows(bigram_counts, .id = "document")
View(bigram_counts_df)
dtm_bigrams <- bigram_counts_df %>% cast_dtm(document = "bigram", term = "bigram", value = "n")
k <- 5
lda_model <- LDA(dtm_bigrams, k = k, control = list(seed = 1234))
terms(lda_model, 10)
topics <- tidy(lda_model, matrix = "beta")
topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
ggplot(aes(beta, fct_reorder(term, beta), fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
labs(x = "Beta", y = NULL)
View(dtm_bigrams)
View(bigram_counts_df)
bigram_counts_df <- bind_rows(bigram_counts, .id = "document")
View(bigram_counts)
dtm_bigrams <- bigram_counts_df %>% cast_dtm(document = "bigram", term = "bigram", value = "n")
View(bigram_counts_df)
View(bigram_counts_df)
View(bigram_counts)
View(preprocessed_pdf_files_bigrams)
bigram_counts_df <- bind_rows(bigram_counts, .id = "document")
dtm_bigrams <- bigram_counts_df %>% cast_dtm(document = "document", term = "bigram", value = "n")
View(bigram_counts)
View(dtm_bigrams)
lda_model <- LDA(dtm_bigrams, k = k, control = list(seed = 1234))
terms(lda_model, 10)
View(bigram_counts)
head(bigram_counts_df)
bigram_counts <- list()
for (i in 1:length(preprocessed_pdf_files_bigrams)){
bigram_counts[[i]] <- tibble(text = preprocessed_pdf_files_bigrams[[i]]) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
count(bigram, sort = TRUE)
}
bigram_counts_df <- bind_rows(bigram_counts, .id = "document")
dtm_bigrams <- bigram_counts_df %>% cast_dtm(document = "document", term = "bigram", value = "n")
View(dtm_bigrams)
lda_model <- LDA(dtm_bigrams, k = k, control = list(seed = 1234))
terms(lda_model, 10)
topics <- tidy(lda_model, matrix = "beta")
topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
ggplot(aes(beta, fct_reorder(term, beta), fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
labs(x = "Beta", y = NULL)
setwd("C:/Users/Joanna/Desktop/programowanie/r/nlp-for-papers")
source("LDA.R")
source("preprocessing.R")
preprocessed_pdf_files_bigrams <- preprocessing_of_pdf_files(pdf_files = pdf_files, bigrams = TRUE)
bigram_counts <- list()
for (i in 1:length(preprocessed_pdf_files_bigrams)){
bigram_counts[[i]] <- tibble(text = preprocessed_pdf_files_bigrams[[i]]) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
count(bigram, sort = TRUE)
}
LDA_modelling(bigram_counts, TRUE)
preprocessed_pdf_files <- preprocessing_of_pdf_files(pdf_files = pdf_files, bigrams = FALSE)
word_counts <- list()
for (i in 1:length(preprocessed_pdf_files)){
word_counts[[i]] <- tibble(text = preprocessed_pdf_files[[i]]) %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE)
}
ggplot(word_counts, aes(x = reorder(word,-n), y = n)) + geom_bar(stat = "identity") + labs(title = "Filtered words", x = "Word", y = "Number")
View(word_counts)
ggplot(word_counts[[1]], aes(x = reorder(word,-n), y = n)) + geom_bar(stat = "identity") + labs(title = "Filtered words", x = "Word", y = "Number")
words_filtered <- word_counts[[1]] %>% filter(freq > 0.005 & freq <= 0.02)
words_filtered <- mutate(word_counts[[1]], freq = n/sum(n))
words_filtered <- words_filtered %>% filter(freq > 0.005 & freq <= 0.02)
ggplot(words_filtered, aes(x = reorder(word,-n), y = n)) + geom_bar(stat = "identity") + labs(title = "Filtered words", x = "Word", y = "Number")
wordcloud(words = words_filtered$word, freq = words_filtered$n, min.freq = 0.005, max.words=30, colors = brewer.pal(3, "Set2"))
wordcloud2(data = words_filtered, size = 0.8, color = "white", backgroundColor = "blue")
wordcloud2(data = bigram_counts[[1]], size = 0.8, color = "white", backgroundColor = "red")
bigram_counts_sep <- preprocessed_pdf_files_bigrams[[1]] %>% count(word1, word2, sort = TRUE)
